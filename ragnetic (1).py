# -*- coding: utf-8 -*-
"""RAGNETIC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LcHe0PqZ0JkKyXfMhF63GgGrMDfYVJKI
"""

!pip install -U \
  weaviate-client \
  langchain \
  langchain-community \
  langchain-openai \
  langchain-weaviate \
  langchain-huggingface \
  tiktoken \
  pypdf \
  rapidocr-onnxruntime

!pip install -U huggingface_hub

WEAVIATE_API_KEY="QKtB9W5SMsV100en8bDhO8xI8WVLpx1g6o4H"
WEAVIATE_CLUSTER="fmzic2jmqko6i2tlsmoaq.c0.us-west3.gcp.weaviate.cloud"

from langchain_weaviate.vectorstores import WeaviateVectorStore
import weaviate
from weaviate.classes.init import Auth
WEAVIATE_URL=WEAVIATE_CLUSTER
WEAVIATE_API_KEY=WEAVIATE_API_KEY

# Connect to Weaviate Cloud
client = weaviate.connect_to_weaviate_cloud(
    cluster_url=WEAVIATE_URL,
    auth_credentials=Auth.api_key(WEAVIATE_API_KEY),
)

print(client.is_ready())

!pip install -U sentence-transformers transformers

#the purpose of this is to fix unicode problem in google colab
import locale
locale.getpreferredencoding = lambda:"UTF-8"

import sentence_transformers
print(sentence_transformers.__version__)

# specify embedding model (using huggingface sentence transformer)
from langchain_huggingface import HuggingFaceEmbeddings
model_name = "sentence-transformers/all-mpnet-base-v2"
# model_kwargs = {'device': 'cpu'}
# encode_kwargs = {'normalize_embeddings': False}
hf = HuggingFaceEmbeddings(
    model_name=model_name,
    # model_kwargs=model_kwargs,
    # encode_kwargs=encode_kwargs
)

#pdf Loader
from langchain_community.document_loaders import PyPDFLoader
loader = PyPDFLoader("/content/Forecasting_at_scale.pdf",extract_images=True)
pages= loader.load()
print(pages[1].page_content)

# split text into chunks
from langchain_text_splitters import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(pages)

texts

import re
from langchain.schema import Document

def sanitize_property_name(name):
    name = re.sub(r'[^_0-9A-Za-z]', '_', name)
    if not re.match(r'^[_A-Za-z]', name):
        name = '_' + name
    return name[:230]

def sanitize_metadata(doc: Document):
    clean_metadata = {
        sanitize_property_name(k): v for k, v in doc.metadata.items()
    }
    return Document(page_content=doc.page_content, metadata=clean_metadata)

# Apply sanitization
texts = [sanitize_metadata(doc) for doc in texts]

from langchain_weaviate.vectorstores import WeaviateVectorStore

vector_db = WeaviateVectorStore.from_documents(
    texts, hf, client=client, by_text=False
)

print(vector_db.similarity_search("what is forecasting?", k=3))

results = vector_db.similarity_search("what is forecasting?", k=3)

for j, doc in enumerate(results[0:3], start=1):
    print(f"Number: {j}")
    print(doc.page_content)
    print("-" * 80)

from langchain_core.prompts import ChatPromptTemplate

template="""You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.
Use nine sentences maximum and keep the answer concise.
Question: {question}
Context: {context}
Answer:
"""

prompt=ChatPromptTemplate.from_template(template)

from langchain_community.llms import HuggingFaceEndpoint

# from google.colab import userdata
# huggingface_hub_api_token=userdata.get('HUGGINGFACE_API_TOKEN')

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline

model_id = "mistralai/Mistral-7B-Instruct-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, device_map="auto")

pipe = pipeline("text-generation", model=model, tokenizer=tokenizer, max_new_tokens=180)

llm = HuggingFacePipeline(pipeline=pipe)

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

output_parser=StrOutputParser()
retriever = vector_db.as_retriever()

rag_chain = (
    {"context": retriever,  "question": RunnablePassthrough()}
    | prompt
    | llm
    | output_parser
)

print(rag_chain.invoke("what is forecasting"))